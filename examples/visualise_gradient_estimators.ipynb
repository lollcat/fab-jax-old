{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import distrax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will see in this notebook:\n",
    "\n",
    " - estimating gradients over q can sometimes provide very poor approximations for mass covering losses such as alpha divergence with alpha=2. \n",
    " - this can be improved by estimating the gradient over p instead\n",
    " - using annealed importance sampling, we can get \"closer\" to p, with an importance sampling correction to account for how far away p we are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_p = distrax.Normal(loc=-loc, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_q_init = jnp.array(loc)\n",
    "dist_q = distrax.Normal(loc=mean_q_init, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-8, 8, 50)\n",
    "plt.plot(x, jnp.exp(dist_q.log_prob(x)), label=\"q\")\n",
    "plt.plot(x, jnp.exp(dist_p.log_prob(x)), label=\"p\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss (alpha-2 divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(dist_p, dist_q, x):\n",
    "    return jnp.exp(2*dist_p.log_prob(x) - dist_q.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "x = jnp.linspace(-8, 8, 50)\n",
    "axs[0].plot(x, loss(dist_p, dist_q, x), label=\"p^2/q\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(x, jnp.exp(dist_q.log_prob(x)), label=\"q\")\n",
    "axs[1].plot(x, jnp.exp(dist_p.log_prob(x)), label=\"p\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate loss function over p vs over q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_estimate_loss_p(dist_p, dist_q, batch_size, key):\n",
    "    x, log_p = dist_p.sample_and_log_prob(seed=key, sample_shape=(batch_size,))\n",
    "    log_q = dist_q.log_prob(x)\n",
    "    return jnp.mean(jnp.exp(log_p - log_q))\n",
    "\n",
    "def mc_estimate_loss_q(dist_p, dist_q, batch_size, key):\n",
    "    x, log_q = dist_q.sample_and_log_prob(seed=key, sample_shape=(batch_size,))\n",
    "    log_p = dist_p.log_prob(x)\n",
    "    return jnp.mean(jnp.exp(2*log_p - 2*log_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [100, 1000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "total_samples = batch_sizes[-1] * 50\n",
    "for batch_size in batch_sizes:\n",
    "    n_runs = total_samples // batch_size\n",
    "    key_batch = jax.random.split(key, n_runs)\n",
    "    losses = np.asarray(jax.vmap(mc_estimate_loss_p, \n",
    "                                 in_axes=(None, None, None, 0))(\n",
    "        dist_p, dist_q, batch_size, key_batch))\n",
    "    loss_hist.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist_q = []\n",
    "for batch_size in batch_sizes:\n",
    "    n_runs = total_samples // batch_size\n",
    "    key_batch = jax.random.split(key, n_runs)\n",
    "    losses_q = np.asarray(jax.vmap(mc_estimate_loss_q, \n",
    "                                 in_axes=(None, None, None, 0))(\n",
    "        dist_p, dist_q, batch_size, key_batch))\n",
    "    loss_hist_q.append(losses_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(batch_sizes, loss_hist, ax, c=\"b\", label=\"\"):\n",
    "    means = np.array([np.mean(loss_hist[i]) for i in range(len(loss_hist))])\n",
    "    stds = np.array([np.std(loss_hist[i]) for i in range(len(loss_hist))])\n",
    "    ax.plot(batch_sizes, means, color=c, label=label)\n",
    "    ax.fill_between(batch_sizes, means - stds, means + stds, alpha=0.1, color=c)\n",
    "    ax.set_xscale(\"log\")\n",
    "    # ax.set_yscale(\"log\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot(batch_sizes, loss_hist, ax, label=\"over p\")\n",
    "plot(batch_sizes, loss_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate grad of loss function over p vs over q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_over_p(mean_q, batch_size, key):\n",
    "    dist_q = distrax.Normal(loc=mean_q, scale=1)\n",
    "    x, log_p = dist_p.sample_and_log_prob(seed=key, sample_shape=(batch_size,))\n",
    "    log_q = dist_q.log_prob(x)\n",
    "    return jnp.mean(jnp.exp(log_p - log_q))\n",
    "\n",
    "\n",
    "def loss_over_q(mean_q, batch_size, key):\n",
    "    dist_q = distrax.Normal(loc=mean_q, scale=1)\n",
    "    x, log_q = dist_q.sample_and_log_prob(seed=key, sample_shape=(batch_size,))\n",
    "    log_p = dist_p.log_prob(x)\n",
    "    return jnp.mean(jnp.exp(2*log_p - 2*log_q))\n",
    "\n",
    "\n",
    "def grad_over_p(mean, batch_size, key):\n",
    "    return jax.grad(loss_over_p)(mean, batch_size, key)\n",
    "\n",
    "def grad_over_q(mean, batch_size, key):\n",
    "    return jax.grad(loss_over_q)(mean, batch_size, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_hist_p = []\n",
    "grad_hist_q = []\n",
    "total_samples = batch_sizes[-1] * 50\n",
    "mean_q = 0.5\n",
    "for batch_size in batch_sizes:\n",
    "    n_runs = total_samples // batch_size\n",
    "    key_batch = jax.random.split(key, n_runs)\n",
    "    grad_p = np.asarray(jax.vmap(grad_over_p, \n",
    "                                 in_axes=(None, None, 0))(\n",
    "        mean_q, batch_size, key_batch))\n",
    "    grad_q = np.asarray(jax.vmap(grad_over_q, \n",
    "                             in_axes=(None, None, 0))(\n",
    "    mean_q, batch_size, key_batch))\n",
    "    grad_hist_p.append(grad_p)\n",
    "    grad_hist_q.append(grad_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot(batch_sizes, grad_hist_p, ax=ax, label=\"over p\")\n",
    "plot(batch_sizes, grad_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_snr(batch_sizes, loss_hist, ax, c=\"b\", label=\"\"):\n",
    "    means = np.array([np.mean(loss_hist[i]) for i in range(len(loss_hist))])\n",
    "    stds = np.array([np.std(loss_hist[i]) for i in range(len(loss_hist))])\n",
    "    ax.plot(batch_sizes, means/stds, color=c, label=label)\n",
    "    ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_snr(batch_sizes, grad_hist_p, ax=ax, label=\"over p\")\n",
    "plot_snr(batch_sizes, grad_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's introduce some AIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fab.sampling_methods.annealed_importance_sampling import AnnealedImportanceSampler\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ais_dist = 3\n",
    "ais = AnnealedImportanceSampler(\n",
    "    dim=1, n_intermediate_distributions=n_ais_dist)\n",
    "transition_operator_state = ais.transition_operator_manager.get_init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to define x with an event dimension to use the AnnealedImportanceSampler\n",
    "dist_p = distrax.Independent(distrax.Normal(loc=[-loc], scale=1), reinterpreted_batch_ndims=1)\n",
    "dist_q = distrax.Independent(distrax.Normal(loc=[loc], scale=1), reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=2)\n",
    "def ais_forward(mean, key, batch_size, transition_operator_state):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    dist_q = distrax.Independent(distrax.Normal(loc=[mean], scale=1), reinterpreted_batch_ndims=1)\n",
    "    base_log_prob = dist_q.log_prob\n",
    "    target_log_prob = dist_p.log_prob\n",
    "    x, log_q = dist_q.sample_and_log_prob(seed=key1, sample_shape=(batch_size,))\n",
    "    x_ais, log_w_ais, new_transition_operator_state, info = ais.run(\n",
    "        x, log_q,\n",
    "     key2,\n",
    "     transition_operator_state,\n",
    "     base_log_prob=base_log_prob,\n",
    "     target_log_prob=target_log_prob)\n",
    "    return x_ais, log_w_ais, new_transition_operator_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    _, _, transition_operator_state = ais_forward(\n",
    "        0.5, key, batch_size=100, transition_operator_state=transition_operator_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ais, log_w_ais, _ = ais_forward(0.5, key, batch_size=100, transition_operator_state=transition_operator_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see the ais distribution is closer to the target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "x = jnp.linspace(-8, 8, 50)[:, None]\n",
    "ax.plot(x, jnp.squeeze(loss(dist_p, dist_q, x)), label=\"p^2/q\")\n",
    "ax.plot(x, jnp.exp(dist_q.log_prob(x)), label=\"q\")\n",
    "ax.plot(x, jnp.exp(dist_p.log_prob(x)), label=\"p\")\n",
    "ax.hist(np.squeeze(x_ais), bins=20, density=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's see how this effects the estimation of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_jitted = jax.jit(dist_p.log_prob)\n",
    "\n",
    "@jax.jit\n",
    "def log_q_jitted(mean, x):\n",
    "    dist_q = distrax.Independent(distrax.Normal(loc=[mean], scale=1), reinterpreted_batch_ndims=1)\n",
    "    return dist_q.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ais_estimate_loss(mean, key, batch_size):\n",
    "    n_samples_inner = 100\n",
    "    # assert batch_size % n_samples_inner == 0\n",
    "    x_ais_list, log_w_ais_list, f_x_term_list = [], [], []\n",
    "    for i in range(batch_size // n_samples_inner):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x_ais, log_w_ais, _ = ais_forward(mean, subkey, n_samples_inner, transition_operator_state)\n",
    "        x_ais_list.append(x_ais)\n",
    "        log_w_ais_list.append(log_w_ais)\n",
    "        log_p_x = log_p_jitted(x_ais)\n",
    "        log_q_x = log_q_jitted(mean, x_ais)\n",
    "        f_x_term = jnp.exp(log_p_x - log_q_x)\n",
    "        f_x_term_list.append(f_x_term)\n",
    "    log_w_ais = jnp.concatenate(log_w_ais_list)\n",
    "    f_x = jnp.concatenate(f_x_term_list)\n",
    "    return jnp.sum(jax.nn.softmax(log_w_ais) * f_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_estimate_loss(mean_q, key, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ais_hist = []\n",
    "mean_q = 0.5\n",
    "for batch_size in batch_sizes:\n",
    "    n_runs = total_samples // batch_size\n",
    "    key_batch = jax.random.split(key, n_runs)\n",
    "    loss_ais = np.asarray(jax.vmap(ais_estimate_loss, \n",
    "                                 in_axes=(None, 0, None))(mean_q, key_batch, batch_size))\n",
    "    loss_ais_hist.append(loss_ais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot(batch_sizes, loss_hist, ax, label=\"over p\")\n",
    "plot(batch_sizes, loss_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "plot(batch_sizes, loss_ais_hist, ax=ax, c=\"g\", label=\"with ais\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let's see how this effects the estimation of the gradient of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ais_get_info(mean, key, batch_size):\n",
    "    n_samples_inner = 100\n",
    "    # assert batch_size % n_samples_inner == 0\n",
    "    x_ais_list, log_w_ais_list= [], []\n",
    "    for i in range(batch_size // n_samples_inner):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x_ais, log_w_ais, _ = ais_forward(mean, subkey, n_samples_inner, transition_operator_state)\n",
    "        x_ais_list.append(x_ais)\n",
    "        log_w_ais_list.append(log_w_ais)\n",
    "    log_w_ais = jnp.concatenate(log_w_ais_list)\n",
    "    x_ais = jnp.concatenate(x_ais_list)\n",
    "    return log_w_ais, x_ais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_inner = 100\n",
    "def grad_with_ais(mean, x_ais, log_w_ais):\n",
    "    log_p_x = dist_p.log_prob(x_ais)\n",
    "    def loss(mean_q):\n",
    "        dist_q = distrax.Independent(distrax.Normal(loc=[mean_q], scale=1), reinterpreted_batch_ndims=1)\n",
    "        log_q_x = dist_q.log_prob(x_ais)\n",
    "        f_x = jnp.exp(log_p_x - log_q_x)\n",
    "        return jnp.sum(jax.nn.softmax(log_w_ais) * f_x)\n",
    "    return jax.value_and_grad(loss)(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [100, 1000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_ais_hist = []\n",
    "mean_q = 0.5\n",
    "total_samples = batch_sizes[-1]*10\n",
    "log_w_ais_all, x_ais_all = ais_get_info(mean_q, key, total_samples)\n",
    "for batch_size in batch_sizes:\n",
    "    n_runs = total_samples // batch_size\n",
    "    key_batch = jax.random.split(key, n_runs)\n",
    "    log_w_ais = jnp.reshape(log_w_ais_all, (n_runs, batch_size))\n",
    "    x_ais = jnp.reshape(x_ais_all, (n_runs, batch_size, 1))\n",
    "    loss_ais, grad_ais = jax.vmap(grad_with_ais, in_axes=(None, 0, 0))(mean_q, x_ais, log_w_ais)\n",
    "    grad_ais_hist.append(grad_ais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "batch_sizes = [100, 1000, 5000]\n",
    "plot(batch_sizes, grad_hist_p, ax=ax, label=\"over p\")\n",
    "plot(batch_sizes, grad_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "plot(batch_sizes, grad_ais_hist, ax=ax, c=\"g\", label=\"with ais\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_snr(batch_sizes, grad_hist_p, ax, label=\"over p\")\n",
    "plot_snr(batch_sizes, grad_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "plot_snr(batch_sizes, grad_ais_hist, ax=ax, c=\"g\", label=\"with ais\")\n",
    "ax.legend()\n",
    "plt.ylabel(\"SNR\")\n",
    "plt.xlabel(\"n_points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about AIS targetting p^2/q?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=2)\n",
    "def ais_forward_alt(mean, key, batch_size, transition_operator_state):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    dist_q = distrax.Independent(distrax.Normal(loc=[mean], scale=1), reinterpreted_batch_ndims=1)\n",
    "    base_log_prob = dist_q.log_prob\n",
    "    target_log_prob = lambda x: 2 * dist_p.log_prob(x) - dist_q.log_prob(x)\n",
    "    x, log_q = dist_q.sample_and_log_prob(seed=key1, sample_shape=(batch_size,))\n",
    "    x_ais, log_w_ais, new_transition_operator_state, info = ais.run(\n",
    "        x, log_q,\n",
    "     key2,\n",
    "     transition_operator_state,\n",
    "     base_log_prob=base_log_prob,\n",
    "     target_log_prob=target_log_prob)\n",
    "    return x_ais, log_w_ais, new_transition_operator_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ais_get_info_alt(mean, key, batch_size):\n",
    "    n_samples_inner = 100\n",
    "    # assert batch_size % n_samples_inner == 0\n",
    "    x_ais_list, log_w_ais_list= [], []\n",
    "    for i in range(batch_size // n_samples_inner):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x_ais, log_w_ais, _ = ais_forward_alt(mean, subkey, n_samples_inner, transition_operator_state)\n",
    "        x_ais_list.append(x_ais)\n",
    "        log_w_ais_list.append(log_w_ais)\n",
    "    log_w_ais = jnp.concatenate(log_w_ais_list)\n",
    "    x_ais = jnp.concatenate(x_ais_list)\n",
    "    return log_w_ais, x_ais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_inner = 100\n",
    "def grad_with_ais_alt(mean, x_ais, log_w_ais, Z):\n",
    "    log_p_x = dist_p.log_prob(x_ais)\n",
    "    def loss(mean_q):\n",
    "        dist_q = distrax.Independent(distrax.Normal(loc=[mean_q], scale=1), reinterpreted_batch_ndims=1)\n",
    "        log_q_x = dist_q.log_prob(x_ais)\n",
    "        f_x =  - log_q_x\n",
    "        return jnp.sum(jax.nn.softmax(log_w_ais) * f_x) * Z\n",
    "    return jax.value_and_grad(loss)(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_ais_hist_alt = []\n",
    "mean_q = 0.5\n",
    "total_samples = batch_sizes[-1]*10\n",
    "log_w_ais_all, x_ais_all = ais_get_info_alt(mean_q, key, total_samples)\n",
    "Z = jnp.exp(jax.nn.logsumexp(log_w_ais_all) - jnp.log(total_samples))\n",
    "for batch_size in batch_sizes:\n",
    "    n_runs = total_samples // batch_size\n",
    "    key_batch = jax.random.split(key, n_runs)\n",
    "    log_w_ais = jnp.reshape(log_w_ais_all, (n_runs, batch_size))\n",
    "    x_ais = jnp.reshape(x_ais_all, (n_runs, batch_size, 1))\n",
    "    loss_ais, grad_ais = jax.vmap(grad_with_ais_alt, in_axes=(None, 0, 0, None))(mean_q, x_ais, log_w_ais, Z)\n",
    "    grad_ais_hist_alt.append(grad_ais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "batch_sizes = [100, 1000, 5000]\n",
    "plot(batch_sizes, grad_hist_p, ax=ax, label=\"over p\")\n",
    "# plot(batch_sizes, grad_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "plot(batch_sizes, grad_ais_hist, ax=ax, c=\"g\", label=\"with ais\")\n",
    "plot(batch_sizes, grad_ais_hist_alt, ax=ax, c=\"r\", label=\"with ais p^2/q\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_snr(batch_sizes, grad_hist_p, ax, label=\"over p\")\n",
    "plot_snr(batch_sizes, grad_hist_q, ax=ax, c=\"r\", label=\"over q\")\n",
    "plot_snr(batch_sizes, grad_ais_hist, ax=ax, c=\"g\", label=\"with ais\")\n",
    "plot_snr(batch_sizes, grad_ais_hist_alt, ax=ax, c=\"black\", label=\"with ais p^2/q\")\n",
    "ax.legend()\n",
    "plt.ylabel(\"SNR\")\n",
    "plt.xlabel(\"n_points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the variance of these estimators change with the number of AIS distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}